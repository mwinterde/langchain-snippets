{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import (\n",
    "    BaseMessage,\n",
    "    HumanMessage, \n",
    "    AIMessage, \n",
    "    SystemMessage \n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Message Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class BaseMessage(BaseModel):\n",
      "    \"\"\"Message object.\"\"\"\n",
      "\n",
      "    content: str\n",
      "    additional_kwargs: dict = Field(default_factory=dict)\n",
      "\n",
      "    @property\n",
      "    @abstractmethod\n",
      "    def type(self) -> str:\n",
      "        \"\"\"Type of the message, used for serialization.\"\"\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# All message types inherit from this class\n",
    "# Each message consists of content (the message itself), a type (who sent the message), \n",
    "# and additional kwargs (which are specific to the API provider)\n",
    "print(inspect.getsource(BaseMessage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class SystemMessage(BaseMessage):\n",
      "    \"\"\"Type of message that is a system message.\"\"\"\n",
      "\n",
      "    @property\n",
      "    def type(self) -> str:\n",
      "        \"\"\"Type of the message, used for serialization.\"\"\"\n",
      "        return \"system\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SystemMessage is a message containing system instructions\n",
    "print(inspect.getsource(SystemMessage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class HumanMessage(BaseMessage):\n",
      "    \"\"\"Type of message that is spoken by the human.\"\"\"\n",
      "\n",
      "    example: bool = False\n",
      "\n",
      "    @property\n",
      "    def type(self) -> str:\n",
      "        \"\"\"Type of the message, used for serialization.\"\"\"\n",
      "        return \"human\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# HumanMessage is a message sent by the user\n",
    "# For few shot examples, we can set the additional attribute `example` to True\n",
    "print(inspect.getsource(HumanMessage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class AIMessage(BaseMessage):\n",
      "    \"\"\"Type of message that is spoken by the AI.\"\"\"\n",
      "\n",
      "    example: bool = False\n",
      "\n",
      "    @property\n",
      "    def type(self) -> str:\n",
      "        \"\"\"Type of the message, used for serialization.\"\"\"\n",
      "        return \"ai\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# AIMessage is a message sent by the chatbot\n",
    "# For few shot examples, we can set the additional attribute `example` to True\n",
    "print(inspect.getsource(AIMessage))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling a Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As an AI language model, I don't have feelings, but I'm functioning well. How can I assist you today?\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Different from LLMs, calling a chat model requires a list of messages, not just\n",
    "# a single prompt, so we wrap our message in a list\n",
    "response = chat_model([\n",
    "    HumanMessage(content=\"Hello, how are you?\")\n",
    "])\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your initial question was \"Hello, how are you?\"', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Passing a list of messages\n",
    "response = chat_model([\n",
    "    HumanMessage(content=\"Hello, how are you?\"),\n",
    "    AIMessage(content=\"As an AI language model, I don't have feelings, but I'm functioning well. How can I assist you today?\"),\n",
    "    HumanMessage(content=\"Can you remind me what my initial question was about?\"),\n",
    "])\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! I'm doing well, thank you. How about you?\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using system messages, we can add general instructions to the chat model\n",
    "response = chat_model([\n",
    "    SystemMessage(content=\"You are a chatbot. You will be small talking with a human. Please be empathic and follow usual social conventions. Don't mention that you are a chatbot.\"),\n",
    "    HumanMessage(content=\"Hello, how are you?\"),\n",
    "])\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[ChatGeneration(text=\"As an AI language model, I don't have feelings, but I'm functioning well. How can I assist you today?\", generation_info=None, message=AIMessage(content=\"As an AI language model, I don't have feelings, but I'm functioning well. How can I assist you today?\", additional_kwargs={}, example=False))], [ChatGeneration(text=\"Hello! I'm doing well, thank you. How about you?\", generation_info=None, message=AIMessage(content=\"Hello! I'm doing well, thank you. How about you?\", additional_kwargs={}, example=False))]], llm_output={'token_usage': {'prompt_tokens': 68, 'completion_tokens': 39, 'total_tokens': 107}, 'model_name': 'gpt-3.5-turbo'})"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similarly to LLMs, we can use the `generate` method to generate multiple responses\n",
    "response = chat_model.generate([\n",
    "    [\n",
    "        HumanMessage(content=\"Hello, how are you?\")\n",
    "    ],\n",
    "    [\n",
    "        SystemMessage(content=\"You are a chatbot. You will be small talking with a human. Please be empathic and follow usual social conventions. Don't mention that you are a chatbot.\"),\n",
    "        HumanMessage(content=\"Hello, how are you?\"),\n",
    "    ]\n",
    "])\n",
    "response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotate Few Shot Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='üíã', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat_model([\n",
    "    SystemMessage(content=\"You are a chatbot that reacts with emojis to input messages of a user. Please don't use words in your responses, only emojis.\"),\n",
    "    HumanMessage(content=\"I like pizza\", example=True),\n",
    "    AIMessage(content=\"üçï\", example=True),\n",
    "    HumanMessage(content=\"The weather is so nice today\", example=True),\n",
    "    AIMessage(content=\"üåû\", example=True),\n",
    "    HumanMessage(content=\"I will go skiing tomorrow\", example=True),\n",
    "    AIMessage(content=\"‚õ∑Ô∏è\", example=True),\n",
    "    HumanMessage(content=\"She gave me a kiss\"),\n",
    "])\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_snippets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
