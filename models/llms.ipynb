{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI, AlephAlpha\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
    "prompt = \"Question: Say something surprising!\\nAnswer: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The sun is actually white, not yellow.\n"
     ]
    }
   ],
   "source": [
    "# Run the model on a single prompt using the __call__ method\n",
    "output = llm(prompt)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The sun is actually white, not yellow.\n"
     ]
    }
   ],
   "source": [
    "# Run the model on a single prompt using the predict method\n",
    "output = llm.predict(prompt)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[Generation(text=' The sun is actually white, not yellow.', generation_info={'finish_reason': 'stop', 'logprobs': None})]] llm_output={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 10, 'total_tokens': 19}, 'model_name': 'text-davinci-003'}\n"
     ]
    }
   ],
   "source": [
    "# Run the model on a single prompt using the generate method\n",
    "output = llm.generate([prompt])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text=' The average human body contains enough iron to make a 3-inch nail.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=' Antibiotics are drugs that kill or inhibit the growth of bacteria.', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 22, 'total_tokens': 51}, 'model_name': 'text-davinci-003'})"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the model on multiple prompts using the generate method\n",
    "second_prompt = \"Question: Explain antibiotics in one sentence.\\nAnswer: \"\n",
    "generations = llm.generate([prompt, second_prompt])\n",
    "generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Generation(text=' The sun is actually white, not yellow.', generation_info={'finish_reason': 'stop', 'logprobs': None})]\n",
      "[Generation(text=' Antibiotics are drugs that kill or inhibit the growth of bacteria.', generation_info={'finish_reason': 'stop', 'logprobs': None})]\n"
     ]
    }
   ],
   "source": [
    "# Access the model output of each prompt\n",
    "print(generations.generations[0])\n",
    "print(generations.generations[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_usage': {'completion_tokens': 23, 'prompt_tokens': 22, 'total_tokens': 45}, 'model_name': 'text-davinci-003'}\n"
     ]
    }
   ],
   "source": [
    "# Access metadata about the model output\n",
    "print(generations.llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def __call__(\n",
      "        self, prompt: str, stop: Optional[List[str]] = None, callbacks: Callbacks = None\n",
      "    ) -> str:\n",
      "        \"\"\"Check Cache and run the LLM on the given prompt and input.\"\"\"\n",
      "        return (\n",
      "            self.generate([prompt], stop=stop, callbacks=callbacks)\n",
      "            .generations[0][0]\n",
      "            .text\n",
      "        )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# __call__ uses the generate method under the hood\n",
    "# It does allow for stop tokens and callbacks, but not for multiple prompts\n",
    "print(inspect.getsource(llm.__call__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def predict(self, text: str, *, stop: Optional[Sequence[str]] = None) -> str:\n",
      "        if stop is None:\n",
      "            _stop = None\n",
      "        else:\n",
      "            _stop = list(stop)\n",
      "        return self(text, stop=_stop)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predict uses the __call__ method under the hood\n",
    "# It does allow for stop tokens, but neither for callbacks nor for multiple prompts\n",
    "print(inspect.getsource(llm.predict))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differences Between LLMs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration possibilities can vary from LLM provider to LLM provider. These differences are reflected in the `__init__` method of the respective LLM class. However, after initialisation, the actual interface of each LLM stays the same.\n",
    "\n",
    "To get more details on the configuration possibilities of a specific LLM, it's usually best to refer to the API documentation of the respective LLM provider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cache': None,\n",
       " 'verbose': None,\n",
       " 'callbacks': None,\n",
       " 'callback_manager': None,\n",
       " 'client': None,\n",
       " 'model_name': 'text-davinci-003',\n",
       " 'temperature': 0.7,\n",
       " 'max_tokens': 256,\n",
       " 'top_p': 1,\n",
       " 'frequency_penalty': 0,\n",
       " 'presence_penalty': 0,\n",
       " 'n': 1,\n",
       " 'best_of': 1,\n",
       " 'model_kwargs': None,\n",
       " 'openai_api_key': None,\n",
       " 'openai_api_base': None,\n",
       " 'openai_organization': None,\n",
       " 'batch_size': 20,\n",
       " 'request_timeout': None,\n",
       " 'logit_bias': None,\n",
       " 'max_retries': 6,\n",
       " 'streaming': False,\n",
       " 'allowed_special': set(),\n",
       " 'disallowed_special': 'all'}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print call arguments of the OpenAI class\n",
    "inspect.getcallargs(OpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cache': None,\n",
       " 'verbose': None,\n",
       " 'callbacks': None,\n",
       " 'callback_manager': None,\n",
       " 'client': None,\n",
       " 'model': 'luminous-base',\n",
       " 'maximum_tokens': 64,\n",
       " 'temperature': 0.0,\n",
       " 'top_k': 0,\n",
       " 'top_p': 0.0,\n",
       " 'presence_penalty': 0.0,\n",
       " 'frequency_penalty': 0.0,\n",
       " 'repetition_penalties_include_prompt': False,\n",
       " 'use_multiplicative_presence_penalty': False,\n",
       " 'penalty_bias': None,\n",
       " 'penalty_exceptions': None,\n",
       " 'penalty_exceptions_include_stop_sequences': None,\n",
       " 'best_of': None,\n",
       " 'n': 1,\n",
       " 'logit_bias': None,\n",
       " 'log_probs': None,\n",
       " 'tokens': False,\n",
       " 'disable_optimizations': False,\n",
       " 'minimum_tokens': 0,\n",
       " 'echo': False,\n",
       " 'use_multiplicative_frequency_penalty': False,\n",
       " 'sequence_penalty': 0.0,\n",
       " 'sequence_penalty_min_length': 2,\n",
       " 'use_multiplicative_sequence_penalty': False,\n",
       " 'completion_bias_inclusion': None,\n",
       " 'completion_bias_inclusion_first_token_only': False,\n",
       " 'completion_bias_exclusion': None,\n",
       " 'completion_bias_exclusion_first_token_only': False,\n",
       " 'contextual_control_threshold': None,\n",
       " 'control_log_additive': True,\n",
       " 'repetition_penalties_include_completion': True,\n",
       " 'raw_completion': False,\n",
       " 'aleph_alpha_api_key': None,\n",
       " 'stop_sequences': None}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print call arguments of the AlephAlpha class\n",
    "inspect.getcallargs(AlephAlpha)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built-In Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import OpenAICallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokens Used: 0\n",
       "\tPrompt Tokens: 0\n",
       "\tCompletion Tokens: 0\n",
       "Successful Requests: 0\n",
       "Total Cost (USD): $0.0"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialise a built-in callback handler\n",
    "# OpenAICallbackHandler tracks tokens and costs of calls to the OpenAI API\n",
    "callback = OpenAICallbackHandler()\n",
    "callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokens Used: 19\n",
       "\tPrompt Tokens: 10\n",
       "\tCompletion Tokens: 9\n",
       "Successful Requests: 1\n",
       "Total Cost (USD): $0.00038"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(prompt, callbacks=[callback])\n",
    "callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokens Used: 38\n",
       "\tPrompt Tokens: 20\n",
       "\tCompletion Tokens: 18\n",
       "Successful Requests: 2\n",
       "Total Cost (USD): $0.00076"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(prompt, callbacks=[callback])\n",
    "callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokens Used: 19\n",
       "\tPrompt Tokens: 10\n",
       "\tCompletion Tokens: 9\n",
       "Successful Requests: 1\n",
       "Total Cost (USD): $0.00038"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also attach the callback handler directly to the LLM with the __call__ method\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0, callbacks=[OpenAICallbackHandler()])\n",
    "llm(prompt)\n",
    "llm.callbacks[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "\n",
    "\n",
    "class CharacterCountTracker(BaseCallbackHandler):\n",
    "    \"\"\"Counts the number of characters in the model output.\"\"\"\n",
    "    \n",
    "    total_characters: int = 0\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Total Characters: {self.total_characters}\"\n",
    "    \n",
    "    def on_llm_end(self, response: object, **kwargs: Any) -> Any:\n",
    "        \"\"\"Run when LLM ends running.\"\"\"\n",
    "        for generation in response.generations:\n",
    "            self.total_characters += len(generation[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Total Characters: 0"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0, callbacks=[CharacterCountTracker()])\n",
    "llm.callbacks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Total Characters: 39"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(prompt)\n",
    "llm.callbacks[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The sun is actually white, not yellow."
     ]
    }
   ],
   "source": [
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0, streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n",
    "llm(prompt);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the idea of streaming is difficult to illustrate by publishing a static notebook,\n",
    "# let's define a custom callback handler that for each token of the response prints\n",
    "# the timestamp when it was received.\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "class PrintTimestampOfNewToken(BaseCallbackHandler):\n",
    "    \"\"\"Prints the timestamp of each new token. Only available when streaming is enabled.\"\"\"\n",
    "    \n",
    "    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:\n",
    "        \"\"\"Run on new LLM token. Only available when streaming is enabled.\"\"\"\n",
    "        now = datetime.now().strftime(\"%H:%M:%S.%f\")\n",
    "        print(f\"{now}: {token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21:11:54.358412:  The\n",
      "21:11:54.416178:  sun\n",
      "21:11:54.542163:  is\n",
      "21:11:54.620219:  actually\n",
      "21:11:55.583465:  white\n",
      "21:11:55.583881: ,\n",
      "21:11:55.584171:  not\n",
      "21:11:55.760662:  yellow\n",
      "21:11:55.863890: .\n",
      "21:11:55.905079: \n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0, streaming=True, callbacks=[PrintTimestampOfNewToken()])\n",
    "llm(prompt);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_snippets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
